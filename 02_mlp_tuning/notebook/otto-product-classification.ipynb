{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Dive Into Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recap\n",
    "\n",
    "- ### Multi-layer Perceptron (MLP)\n",
    "- ### Automatic feature extraction\n",
    "- ### Prediction function, loss function\n",
    "- ### Optimization algorithm, optimization method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Goals Of This Class\n",
    "\n",
    "- ### Advanced classification using softmax/cross-entropy\n",
    "- ### Understanding of backpropagation algorithm\n",
    "- ### Hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dataset background\n",
    "\n",
    "The Otto Group is one of the world’s biggest e-commerce companies. For Otto group, a consistent analysis of the performance of products is crucial. However, due to diverse global infrastructure, many identical products get classified differently. For this problem, we have a dataset of 200,000 products with 93 features, and each product is classified into one of the 9 categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The objective is to build a predictive model which is able to distinguish between 9 main product categories. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import seaborn as sns\n",
    "import keras \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Changes wrt the previous problem\n",
    "\n",
    "- ### More features, larger dataset\n",
    "- ### Multi-class classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Binary vs Multiclass Classification\n",
    "\n",
    "<img src=\"../images/binary-vs-multiclass-0.png\" width=\"70%\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, train=True):\n",
    "    \"\"\"Load data from a CSV File\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path: str\n",
    "        The path to the CSV file\n",
    "        \n",
    "    train: bool (default True)\n",
    "        Decide whether or not data are *training data*.\n",
    "        If True, some random shuffling is applied.\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    X: numpy.ndarray \n",
    "        The data as a multi dimensional array of floats\n",
    "    ids: numpy.ndarray\n",
    "        A vector of ids for each sample\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    \n",
    "    X = df.values.copy()\n",
    "    if train:\n",
    "        np.random.shuffle(X)\n",
    "        X, labels = X[:, 1:-1].astype(np.float32), X[:, -1]\n",
    "        return X, labels, df\n",
    "    else:\n",
    "        X, ids = X[:, 1:].astype(np.float32), X[:, 0].astype(str)\n",
    "        return X, ids, df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X, scaler=None):\n",
    "    \"\"\"Preprocess input data by standardise features \n",
    "    by removing the mean and scaling to unit variance\"\"\"\n",
    "    if not scaler:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X)\n",
    "    X = scaler.transform(X)\n",
    "    return X, scaler\n",
    "\n",
    "\n",
    "def preprocess_labels(labels, encoder=None, categorical=True):\n",
    "    \"\"\"Encode labels with values among 0 and `n-classes-1`\"\"\"\n",
    "    if not encoder:\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.fit(labels)\n",
    "    y = encoder.transform(labels).astype(np.int32)\n",
    "    if categorical:\n",
    "        y = np_utils.to_categorical(y)\n",
    "    return y, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_84</th>\n",
       "      <th>feat_85</th>\n",
       "      <th>feat_86</th>\n",
       "      <th>feat_87</th>\n",
       "      <th>feat_88</th>\n",
       "      <th>feat_89</th>\n",
       "      <th>feat_90</th>\n",
       "      <th>feat_91</th>\n",
       "      <th>feat_92</th>\n",
       "      <th>feat_93</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.00000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>30939.500000</td>\n",
       "      <td>0.38668</td>\n",
       "      <td>0.263066</td>\n",
       "      <td>0.901467</td>\n",
       "      <td>0.779081</td>\n",
       "      <td>0.071043</td>\n",
       "      <td>0.025696</td>\n",
       "      <td>0.193704</td>\n",
       "      <td>0.662433</td>\n",
       "      <td>1.011296</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070752</td>\n",
       "      <td>0.532306</td>\n",
       "      <td>1.128576</td>\n",
       "      <td>0.393549</td>\n",
       "      <td>0.874915</td>\n",
       "      <td>0.457772</td>\n",
       "      <td>0.812421</td>\n",
       "      <td>0.264941</td>\n",
       "      <td>0.380119</td>\n",
       "      <td>0.126135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>17862.784315</td>\n",
       "      <td>1.52533</td>\n",
       "      <td>1.252073</td>\n",
       "      <td>2.934818</td>\n",
       "      <td>2.788005</td>\n",
       "      <td>0.438902</td>\n",
       "      <td>0.215333</td>\n",
       "      <td>1.030102</td>\n",
       "      <td>2.255770</td>\n",
       "      <td>3.474822</td>\n",
       "      <td>...</td>\n",
       "      <td>1.151460</td>\n",
       "      <td>1.900438</td>\n",
       "      <td>2.681554</td>\n",
       "      <td>1.575455</td>\n",
       "      <td>2.115466</td>\n",
       "      <td>1.527385</td>\n",
       "      <td>4.597804</td>\n",
       "      <td>2.045646</td>\n",
       "      <td>0.982385</td>\n",
       "      <td>1.201720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>15470.250000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>30939.500000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>46408.750000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61.00000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>87.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 94 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id       feat_1        feat_2        feat_3        feat_4  \\\n",
       "count  61878.000000  61878.00000  61878.000000  61878.000000  61878.000000   \n",
       "mean   30939.500000      0.38668      0.263066      0.901467      0.779081   \n",
       "std    17862.784315      1.52533      1.252073      2.934818      2.788005   \n",
       "min        1.000000      0.00000      0.000000      0.000000      0.000000   \n",
       "25%    15470.250000      0.00000      0.000000      0.000000      0.000000   \n",
       "50%    30939.500000      0.00000      0.000000      0.000000      0.000000   \n",
       "75%    46408.750000      0.00000      0.000000      0.000000      0.000000   \n",
       "max    61878.000000     61.00000     51.000000     64.000000     70.000000   \n",
       "\n",
       "             feat_5        feat_6        feat_7        feat_8        feat_9  \\\n",
       "count  61878.000000  61878.000000  61878.000000  61878.000000  61878.000000   \n",
       "mean       0.071043      0.025696      0.193704      0.662433      1.011296   \n",
       "std        0.438902      0.215333      1.030102      2.255770      3.474822   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      1.000000      0.000000   \n",
       "max       19.000000     10.000000     38.000000     76.000000     43.000000   \n",
       "\n",
       "           ...            feat_84       feat_85       feat_86       feat_87  \\\n",
       "count      ...       61878.000000  61878.000000  61878.000000  61878.000000   \n",
       "mean       ...           0.070752      0.532306      1.128576      0.393549   \n",
       "std        ...           1.151460      1.900438      2.681554      1.575455   \n",
       "min        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "25%        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "50%        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "75%        ...           0.000000      0.000000      1.000000      0.000000   \n",
       "max        ...          76.000000     55.000000     65.000000     67.000000   \n",
       "\n",
       "            feat_88       feat_89       feat_90       feat_91       feat_92  \\\n",
       "count  61878.000000  61878.000000  61878.000000  61878.000000  61878.000000   \n",
       "mean       0.874915      0.457772      0.812421      0.264941      0.380119   \n",
       "std        2.115466      1.527385      4.597804      2.045646      0.982385   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        1.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max       30.000000     61.000000    130.000000     52.000000     19.000000   \n",
       "\n",
       "            feat_93  \n",
       "count  61878.000000  \n",
       "mean       0.126135  \n",
       "std        1.201720  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        0.000000  \n",
       "max       87.000000  \n",
       "\n",
       "[8 rows x 94 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "X, labels, dataset = load_data('../data/train.csv', train=True)\n",
    "\n",
    "dataset.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_85</th>\n",
       "      <th>feat_86</th>\n",
       "      <th>feat_87</th>\n",
       "      <th>feat_88</th>\n",
       "      <th>feat_89</th>\n",
       "      <th>feat_90</th>\n",
       "      <th>feat_91</th>\n",
       "      <th>feat_92</th>\n",
       "      <th>feat_93</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  feat_1  feat_2  feat_3  feat_4  feat_5  feat_6  feat_7  feat_8  feat_9  \\\n",
       "0   1       1       0       0       0       0       0       0       0       0   \n",
       "1   2       0       0       0       0       0       0       0       1       0   \n",
       "2   3       0       0       0       0       0       0       0       1       0   \n",
       "3   4       1       0       0       1       6       1       5       0       0   \n",
       "4   5       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "    ...     feat_85  feat_86  feat_87  feat_88  feat_89  feat_90  feat_91  \\\n",
       "0   ...           1        0        0        0        0        0        0   \n",
       "1   ...           0        0        0        0        0        0        0   \n",
       "2   ...           0        0        0        0        0        0        0   \n",
       "3   ...           0        1        2        0        0        0        0   \n",
       "4   ...           1        0        0        0        0        1        0   \n",
       "\n",
       "   feat_92  feat_93   target  \n",
       "0        0        0  Class_1  \n",
       "1        0        0  Class_1  \n",
       "2        0        0  Class_1  \n",
       "3        0        0  Class_1  \n",
       "4        0        0  Class_1  \n",
       "\n",
       "[5 rows x 95 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick look at the data\n",
    "dataset.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot of the distribution of each feature\n",
    "def plot_distribution(dataset, cols=5, width=20, height=15, hspace=0.2, wspace=0.5):\n",
    "    plt.style.use('seaborn-whitegrid')\n",
    "    fig = plt.figure(figsize=(width,height))\n",
    "    fig.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=wspace, hspace=hspace)\n",
    "    rows = math.ceil(float(dataset.shape[1]) / cols)\n",
    "    for i, column in enumerate(dataset.columns):\n",
    "        ax = fig.add_subplot(rows, cols, i + 1)\n",
    "        ax.set_title(column)\n",
    "        if dataset.dtypes[column] == np.object:\n",
    "            g = sns.countplot(y=column, data=dataset)\n",
    "            substrings = [s.get_text()[:18] for s in g.get_yticklabels()]\n",
    "            g.set(yticklabels=substrings)\n",
    "            plt.xticks(rotation=25)\n",
    "        else:\n",
    "            g = sns.distplot(dataset[column])\n",
    "            plt.xticks(rotation=25)\n",
    "    \n",
    "plot_distribution(dataset, cols=10, width=20, height=20, hspace=0.45, wspace=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "93 dims\n",
      "9 classes\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "X, labels, dataset = load_data('../data/train.csv', train=True)\n",
    "X, scaler = preprocess_data(X)\n",
    "Y, encoder = preprocess_labels(labels)\n",
    "\n",
    "X = X[10000:]\n",
    "Y = Y[10000:]\n",
    "X_val = X[:10000]\n",
    "Y_val = Y[:10000]\n",
    "\n",
    "dims = X.shape[1]\n",
    "print(dims, 'dims')\n",
    "nb_classes = Y.shape[1]\n",
    "print(nb_classes, 'classes')\n",
    "\n",
    "X_test, ids, _ = load_data('../data/test.csv', train=False)\n",
    "X_test, ids = X_test[:1000], ids[:1000]\n",
    "X_test, _ = preprocess_data(X_test, scaler)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's create a simple one layer multi-class classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 51878 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "51878/51878 [==============================] - 3s 51us/step - loss: 1.1252 - acc: 0.6513 - val_loss: 0.8296 - val_acc: 0.7265\n",
      "Epoch 2/30\n",
      "51878/51878 [==============================] - 2s 40us/step - loss: 0.7928 - acc: 0.7312 - val_loss: 0.7544 - val_acc: 0.7386\n",
      "Epoch 3/30\n",
      "51878/51878 [==============================] - 2s 40us/step - loss: 0.7436 - acc: 0.7397 - val_loss: 0.7226 - val_acc: 0.7446\n",
      "Epoch 4/30\n",
      "51878/51878 [==============================] - 2s 40us/step - loss: 0.7194 - acc: 0.7437 - val_loss: 0.7047 - val_acc: 0.7476\n",
      "Epoch 5/30\n",
      "51878/51878 [==============================] - 3s 54us/step - loss: 0.7044 - acc: 0.7468 - val_loss: 0.6927 - val_acc: 0.7498\n",
      "Epoch 6/30\n",
      "51878/51878 [==============================] - 2s 40us/step - loss: 0.6941 - acc: 0.7488 - val_loss: 0.6837 - val_acc: 0.7510\n",
      "Epoch 7/30\n",
      "51878/51878 [==============================] - 2s 41us/step - loss: 0.6864 - acc: 0.7499 - val_loss: 0.6777 - val_acc: 0.7529\n",
      "Epoch 8/30\n",
      "51878/51878 [==============================] - 2s 43us/step - loss: 0.6807 - acc: 0.7519 - val_loss: 0.6724 - val_acc: 0.7539\n",
      "Epoch 9/30\n",
      "51878/51878 [==============================] - 3s 49us/step - loss: 0.6759 - acc: 0.7533 - val_loss: 0.6686 - val_acc: 0.7563\n",
      "Epoch 10/30\n",
      "51878/51878 [==============================] - 3s 50us/step - loss: 0.6720 - acc: 0.7538 - val_loss: 0.6658 - val_acc: 0.7556\n",
      "Epoch 11/30\n",
      "51878/51878 [==============================] - 3s 49us/step - loss: 0.6687 - acc: 0.7544 - val_loss: 0.6624 - val_acc: 0.7563\n",
      "Epoch 12/30\n",
      "51878/51878 [==============================] - 2s 42us/step - loss: 0.6659 - acc: 0.7550 - val_loss: 0.6604 - val_acc: 0.7569\n",
      "Epoch 13/30\n",
      "51878/51878 [==============================] - 2s 41us/step - loss: 0.6636 - acc: 0.7565 - val_loss: 0.6579 - val_acc: 0.7586\n",
      "Epoch 14/30\n",
      "51878/51878 [==============================] - 2s 44us/step - loss: 0.6612 - acc: 0.7572 - val_loss: 0.6576 - val_acc: 0.7570\n",
      "Epoch 15/30\n",
      "51878/51878 [==============================] - 2s 39us/step - loss: 0.6596 - acc: 0.7567 - val_loss: 0.6542 - val_acc: 0.7603\n",
      "Epoch 16/30\n",
      "51878/51878 [==============================] - 2s 41us/step - loss: 0.6578 - acc: 0.7576 - val_loss: 0.6527 - val_acc: 0.7600\n",
      "Epoch 17/30\n",
      "51878/51878 [==============================] - 3s 50us/step - loss: 0.6562 - acc: 0.7589 - val_loss: 0.6517 - val_acc: 0.7603\n",
      "Epoch 18/30\n",
      "51878/51878 [==============================] - 2s 46us/step - loss: 0.6551 - acc: 0.7581 - val_loss: 0.6501 - val_acc: 0.7598\n",
      "Epoch 19/30\n",
      "51878/51878 [==============================] - 2s 41us/step - loss: 0.6537 - acc: 0.7586 - val_loss: 0.6497 - val_acc: 0.7601\n",
      "Epoch 20/30\n",
      "51878/51878 [==============================] - 2s 42us/step - loss: 0.6526 - acc: 0.7594 - val_loss: 0.6490 - val_acc: 0.7608\n",
      "Epoch 21/30\n",
      "51878/51878 [==============================] - 2s 43us/step - loss: 0.6517 - acc: 0.7591 - val_loss: 0.6473 - val_acc: 0.7624\n",
      "Epoch 22/30\n",
      "51878/51878 [==============================] - 2s 41us/step - loss: 0.6505 - acc: 0.7601 - val_loss: 0.6466 - val_acc: 0.7616\n",
      "Epoch 23/30\n",
      "51878/51878 [==============================] - 2s 38us/step - loss: 0.6496 - acc: 0.7597 - val_loss: 0.6462 - val_acc: 0.7618\n",
      "Epoch 24/30\n",
      "51878/51878 [==============================] - 2s 42us/step - loss: 0.6489 - acc: 0.7598 - val_loss: 0.6449 - val_acc: 0.7628\n",
      "Epoch 25/30\n",
      "51878/51878 [==============================] - 3s 66us/step - loss: 0.6480 - acc: 0.7597 - val_loss: 0.6442 - val_acc: 0.7649\n",
      "Epoch 26/30\n",
      "51878/51878 [==============================] - 3s 55us/step - loss: 0.6474 - acc: 0.7606 - val_loss: 0.6438 - val_acc: 0.7630\n",
      "Epoch 27/30\n",
      "51878/51878 [==============================] - 3s 49us/step - loss: 0.6467 - acc: 0.7608 - val_loss: 0.6430 - val_acc: 0.7620\n",
      "Epoch 28/30\n",
      "51878/51878 [==============================] - 2s 46us/step - loss: 0.6461 - acc: 0.7606 - val_loss: 0.6425 - val_acc: 0.7638\n",
      "Epoch 29/30\n",
      "51878/51878 [==============================] - 2s 47us/step - loss: 0.6455 - acc: 0.7614 - val_loss: 0.6421 - val_acc: 0.7640\n",
      "Epoch 30/30\n",
      "51878/51878 [==============================] - 3s 48us/step - loss: 0.6449 - acc: 0.7611 - val_loss: 0.6424 - val_acc: 0.7632\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a1fc400b8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "# gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.7)\n",
    "# sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(nb_classes, input_shape=(dims,)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X, Y, epochs=30, batch_size=32, validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 17us/step\n",
      "[0.6424088631629944, 0.7632]\n"
     ]
    }
   ],
   "source": [
    "# Part 3 - Making the predictions and evaluating the model\n",
    "\n",
    "# Predicting the Test set results\n",
    "# Training set, see if the new data probability is right\n",
    "results = model.evaluate(X_val, Y_val)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "output shape of first test sample (9,)\n",
      "\n",
      "\n",
      "sum over predictions 0.9999999\n",
      "\n",
      "\n",
      "actual predictions\n",
      " [4.8122888e-06 6.1857444e-01 3.6134896e-01 1.8463681e-02 9.9824998e-04\n",
      " 2.6941436e-06 5.9060508e-04 5.4218312e-06 1.1036260e-05]\n",
      "\n",
      "\n",
      "predicted class 1\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_val)\n",
    "print(\"\\n\\noutput shape of first test sample\", predictions[0].shape)\n",
    "print(\"\\n\\nsum over predictions\",np.sum(predictions[0]))\n",
    "print(\"\\n\\nactual predictions\\n\",predictions[0])\n",
    "print(\"\\n\\npredicted class\",np.argmax(predictions[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's understand the multi-class classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Binary vs Multiclass Classification\n",
    "\n",
    "<img src=\"../images/binary-vs-multiclass.png\" width=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Multi-class vs multi-label classification\n",
    "\n",
    "<img src=\"../images/multiclass-vs-multilabel.png\" width=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How does our current network look?\n",
    "\n",
    "<img src=\"../images/softmax-cross-entropy.png\" width=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/cross-entropy-1.png\" width=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/Maths-Insight.png\" alt=\"Math Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "## Let's understand the prediction function and loss function in some more details.\n",
    "***\n",
    "***\n",
    "\n",
    "\n",
    "<img src=\"../images/softmax-cross-entropy-1.png\" width=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How does cross-entropy help in capturing the classification loss?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Two different probability distributions but same output. But which distribution has lesser uncertainty?\n",
    "\n",
    "<img src=\"../images/cross-entropy-2.png\" width=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Difference between two probability distributions. The second probability distribution has lesser uncertainty with respect to the target distribution as compared to the first distribution.\n",
    "\n",
    "<img src=\"../images/cross-entropy-3.png\" width=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/Recap.png\" alt=\"Recap\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "## Recap\n",
    "***\n",
    "***\n",
    "\n",
    "- A multi-class classification problem\n",
    "- Soft-max as prediction function\n",
    "- Cross-entropy as loss function\n",
    "\n",
    "### Soft-max plus cross-entropy is used across numerous applications for different media types such as image processing, text processing, genome processing etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Y_prediction = []\n",
    "y_prediction = []\n",
    "for prob_array in predictions:\n",
    "    ind = np.argmax(prob_array)\n",
    "    y_prediction.append(ind)\n",
    "    new_prob_array = [0] * len(prob_array)\n",
    "    new_prob_array[ind] = 1\n",
    "    Y_prediction.append(new_prob_array)\n",
    "\n",
    "\n",
    "y_truth = []\n",
    "for prob_array in Y_val:\n",
    "    y_truth.append(np.argmax(prob_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluation metrics\n",
    "\n",
    "- ### Confusion matrix\n",
    "- ### Precision, recall, accuracy\n",
    "- ### F-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix\n",
      "[[ 104   18    0    0    0   33    9   52   68]\n",
      " [   4 2387  196   15   12    4   17   13    4]\n",
      " [   0  941  314   11    1    5   13    4    3]\n",
      " [   0  262   29  116    3   10    5    1    1]\n",
      " [   0   30    0    0  430    0    0    1    0]\n",
      " [  14   29    3    9    1 2090   25   56   30]\n",
      " [   6   84   22    5    3   41  266   42    1]\n",
      " [  20   22    5    0    1   31   14 1258   23]\n",
      " [  21   20    0    0    2   26    6   41  667]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(\"confusion matrix\")\n",
    "cm = confusion_matrix(y_truth, y_prediction)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Precision, Recall, Accuracy, F-score\n",
    "\n",
    "- For class x:\n",
    "  - True positive: diagonal position, cm(x, x).\n",
    "  - False positive: sum of column x (without main diagonal), sum(cm(:, x))-cm(x, x).\n",
    "  - False negative: sum of row x (without main diagonal), sum(cm(x, :))-cm(x, x).\n",
    "\n",
    "- We can compute precision, recall and F1 score following the formulas mentioned below.\n",
    "- Averaging over all classes (with or without weighting) gives values for the entire model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Understand different evaluation metrics\n",
    "\n",
    "<img src=\"../images/metrics.png\" width=\"70%\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " verify true pos\n",
      "[104, 2387, 314, 116, 430, 2090, 266, 1258, 667]\n"
     ]
    }
   ],
   "source": [
    "true_pos = []\n",
    "false_pos = []\n",
    "false_neg = []\n",
    "i = 0\n",
    "for a_class in cm:\n",
    "    true_pos.append(cm[i,i])\n",
    "    false_pos.append(sum(cm[:,i]) - cm[i,i])\n",
    "    false_neg.append(sum(cm[i,:]) - cm[i,i])\n",
    "    i = i + 1\n",
    "\n",
    "print(\"\\n\\n verify true pos\")\n",
    "print(true_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "precision = []\n",
    "recall = []\n",
    "total_precision = 0\n",
    "total_recall = 0\n",
    "total_true = 0\n",
    "total_false = 0\n",
    "accuracy = 0\n",
    "f_score = 0\n",
    "i = 0\n",
    "for a_class in true_pos:\n",
    "    precision.append(true_pos[i]/(true_pos[i]+false_pos[i]))\n",
    "    recall.append(true_pos[i]/(true_pos[i]+false_neg[i]))\n",
    "    total_true = total_true + true_pos[i]\n",
    "    total_false = total_false + false_neg[i]\n",
    "    total_precision = total_precision + precision[i]\n",
    "    total_recall = total_recall + recall[i]\n",
    "    i = i + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " precision per class\n",
      "[0.6153846153846154, 0.6293171631953599, 0.5518453427065027, 0.7435897435897436, 0.9492273730684326, 0.9330357142857143, 0.7492957746478873, 0.8569482288828338, 0.8368883312421581]\n",
      "\n",
      "\n",
      " recall per class\n",
      "[0.36619718309859156, 0.9000754147812972, 0.24303405572755418, 0.2716627634660422, 0.9327548806941431, 0.926007975188303, 0.5659574468085107, 0.9155749636098981, 0.8518518518518519]\n",
      "\n",
      "\n",
      " accuracy\n",
      "0.7632\n",
      "\n",
      "\n",
      " f_score\n",
      "0.7098120508275872\n"
     ]
    }
   ],
   "source": [
    "\n",
    "avg_precision = total_precision/len(true_pos)\n",
    "avg_recall = total_recall/len(true_pos)    \n",
    "accuracy = total_true/(total_false+total_true)\n",
    "f_score = 2 * avg_precision * avg_recall/(avg_precision + avg_recall)\n",
    "print(\"\\n\\n precision per class\")\n",
    "print(precision)\n",
    "print(\"\\n\\n recall per class\")\n",
    "print(recall)\n",
    "print(\"\\n\\n accuracy\")\n",
    "print(accuracy)\n",
    "print(\"\\n\\n f_score\")\n",
    "print(f_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training the ANN with Backpropagation\n",
    "First layer - input layer \n",
    "- Step 1: Randomly initialise the weights to small numbers close to 0 (but not 0)\n",
    "- Step 2: Input the first observation of the dataset in the input layer, each feature in one input node (11 input nodes)\n",
    "- Step 3: Activation function hidden layer (rectifier), sigmoid (good for output layer)\n",
    "- Step 4: Compare the predicted result o the actual result, measure the generated error\n",
    "- Step 5: Back-propagation to update the weights, earning rate decideds how much we update the weights\n",
    "- Step 6: update the weights after a set of observation\n",
    "- Step 7: Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient descent is a special case of backpropagation algorithm\n",
    "\n",
    "<img src=\"../images/backprop-10-1.png\" width=\"70%\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Intuition of backpropagation algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/training-1.png\" width=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/training-2.png\" width=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/training-3.png\" width=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/training-4.png\" width=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/training-5.png\" width=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/training-6.png\" width=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- ### Training deep neural networks involve numerous iterations through the dataset to learn the weights.\n",
    "- ### In each iteration, we pick a random instance or a set of instance, and do the training.\n",
    "- ### We may have to have even millions of iterations depending on the complexity of the problem\n",
    "- ### Each backward pass of training strives to minimize the training error, thus learning a robust classification network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mathematics of backpropagation algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/backprop-1.png\" width=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/backprop-2-1.png\" width=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/backprop-3-1.png\" width=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/backprop-4.png\" width=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<img src=\"../images/backprop-5.png\" width=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's continue with backward pass\n",
    "\n",
    "<img src=\"../images/backprop-6-3.png\" width=\"70%\" align=\"middle\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One more round of backward pass...almost there.\n",
    "\n",
    "<img src=\"../images/backprop-8-1.png\" width=\"70%\" align=\"middle\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/backprop-9-1.png\" width=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/Recap.png\" alt=\"Recap\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "# We now know how deep neural networks learn a function\n",
    "***\n",
    "***\n",
    "\n",
    "\n",
    "- ### define prediction and loss functions\n",
    "- ### do forward pass and compute loss for a sample(or cost over a batch)\n",
    "- ### do backward ass to tune the weights, this is weight training\n",
    "- ### repeat weight training until training loss converges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# But training loss alone does not suffice\n",
    "\n",
    "- ## We want our network to do well in production\n",
    "- ## Typically data is divided between train, validation, and test.\n",
    "- ## use train data to tune the weights\n",
    "- ## use validation data to evaluate the model, and change hyper-parameters\n",
    "- ## use test data, to quantify performance\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/under-over-fitting.png\" width=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Thum rule when training deep neural networks\n",
    "\n",
    "<img src=\"../images/training-nn.png\" width=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What are hyper-parameters\n",
    "\n",
    "- ## Parameters are weights on the edges of the network\n",
    "- ## Hyper-parameters concern about the architecture and training\n",
    "  - ## Depth of the network\n",
    "  - ## Choice of activation function\n",
    "  - ## Learning rate variations\n",
    "  - ## Batch size\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# batch_size vs iteration vs epoch\n",
    "\n",
    "- ## stochastic gradient descent : sample batch size, do forward pass for the batch, compute loss over the batch, then do backward pass\n",
    "- ## iteration : number of times batch size is sampled\n",
    "- ## epoch : number of iterations (one epoch typically signifies one pass over the entire training set)\n",
    "- ## rule of thumb : show one training sample at least 6 to 8 times\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's change hyper parameters\n",
    "\n",
    "- From depth 1 to depth 2 network\n",
    "- Relu as activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "51878/51878 [==============================] - 2s 45us/step - loss: 1.1593 - acc: 0.6418\n",
      "Epoch 2/30\n",
      "51878/51878 [==============================] - 3s 48us/step - loss: 0.7859 - acc: 0.7326\n",
      "Epoch 3/30\n",
      "51878/51878 [==============================] - 3s 58us/step - loss: 0.6969 - acc: 0.7477\n",
      "Epoch 4/30\n",
      "51878/51878 [==============================] - 3s 54us/step - loss: 0.6584 - acc: 0.7563\n",
      "Epoch 5/30\n",
      "51878/51878 [==============================] - 3s 50us/step - loss: 0.6338 - acc: 0.7640\n",
      "Epoch 6/30\n",
      "51878/51878 [==============================] - 3s 59us/step - loss: 0.6163 - acc: 0.7687\n",
      "Epoch 7/30\n",
      "51878/51878 [==============================] - 3s 62us/step - loss: 0.6035 - acc: 0.7719\n",
      "Epoch 8/30\n",
      "51878/51878 [==============================] - 3s 61us/step - loss: 0.5931 - acc: 0.7758\n",
      "Epoch 9/30\n",
      "51878/51878 [==============================] - 3s 48us/step - loss: 0.5847 - acc: 0.7787\n",
      "Epoch 10/30\n",
      "51878/51878 [==============================] - 2s 47us/step - loss: 0.5777 - acc: 0.7813\n",
      "Epoch 11/30\n",
      "51878/51878 [==============================] - 2s 47us/step - loss: 0.5714 - acc: 0.7827\n",
      "Epoch 12/30\n",
      "51878/51878 [==============================] - 2s 46us/step - loss: 0.5658 - acc: 0.7851\n",
      "Epoch 13/30\n",
      "51878/51878 [==============================] - 3s 49us/step - loss: 0.5606 - acc: 0.7865\n",
      "Epoch 14/30\n",
      "51878/51878 [==============================] - 2s 46us/step - loss: 0.5560 - acc: 0.7877\n",
      "Epoch 15/30\n",
      "51878/51878 [==============================] - 2s 46us/step - loss: 0.5517 - acc: 0.7898\n",
      "Epoch 16/30\n",
      "51878/51878 [==============================] - 2s 47us/step - loss: 0.5481 - acc: 0.7916\n",
      "Epoch 17/30\n",
      "51878/51878 [==============================] - 3s 53us/step - loss: 0.5444 - acc: 0.7924\n",
      "Epoch 18/30\n",
      "51878/51878 [==============================] - 3s 49us/step - loss: 0.5410 - acc: 0.7928\n",
      "Epoch 19/30\n",
      "51878/51878 [==============================] - 3s 49us/step - loss: 0.5377 - acc: 0.7941\n",
      "Epoch 20/30\n",
      "51878/51878 [==============================] - 2s 45us/step - loss: 0.5344 - acc: 0.7963\n",
      "Epoch 21/30\n",
      "51878/51878 [==============================] - 2s 45us/step - loss: 0.5318 - acc: 0.7960\n",
      "Epoch 22/30\n",
      "51878/51878 [==============================] - 3s 51us/step - loss: 0.5290 - acc: 0.7976\n",
      "Epoch 23/30\n",
      "51878/51878 [==============================] - 3s 53us/step - loss: 0.5264 - acc: 0.7986\n",
      "Epoch 24/30\n",
      "51878/51878 [==============================] - 3s 51us/step - loss: 0.5240 - acc: 0.7998\n",
      "Epoch 25/30\n",
      "51878/51878 [==============================] - 3s 50us/step - loss: 0.5213 - acc: 0.7999\n",
      "Epoch 26/30\n",
      "51878/51878 [==============================] - 3s 52us/step - loss: 0.5189 - acc: 0.8013\n",
      "Epoch 27/30\n",
      "51878/51878 [==============================] - 2s 47us/step - loss: 0.5168 - acc: 0.8020\n",
      "Epoch 28/30\n",
      "51878/51878 [==============================] - 2s 46us/step - loss: 0.5150 - acc: 0.8028\n",
      "Epoch 29/30\n",
      "51878/51878 [==============================] - 3s 50us/step - loss: 0.5128 - acc: 0.8041\n",
      "Epoch 30/30\n",
      "51878/51878 [==============================] - 3s 51us/step - loss: 0.5110 - acc: 0.8038\n"
     ]
    }
   ],
   "source": [
    "model_1 = Sequential()\n",
    "model_1.add(Dense(64, input_shape=(dims,), activation='relu'))\n",
    "model_1.add(Dense(nb_classes, activation='relu'))\n",
    "model_1.add(Activation('softmax'))\n",
    "model_1.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history=model_1.fit(X, Y, epochs=30, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 25us/step\n",
      "[0.5066415110111236, 0.8049]\n"
     ]
    }
   ],
   "source": [
    "# Part 3 - Making the predictions and evaluating the model\n",
    "\n",
    "# Predicting the Test set results\n",
    "# Training set, see if the new data probability is right\n",
    "results = model_1.evaluate(X_val, Y_val)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ReLU as activation function\n",
    "\n",
    "In the past, nonlinear functions like tanh and sigmoid were used, but researchers found out that **ReLU layers** work far better because the network is able to train a lot faster (because of the computational efficiency) without making a significant difference to the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "ReLu also helps to alleviate the **vanishing gradient problem**, which is the issue where the lower layers of the network train very slowly because the gradient decreases exponentially through the layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Vanishing gradient problem\n",
    "\n",
    "<img src=\"../images/sigmoid-vanishing-gradient.png\" width=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "ReLU: much simpler non-linearity. Constant derivative, faster training.\n",
    "\n",
    "<img src=\"../images/relu-derivative.png\" width=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The **ReLu** function is defined as $f(x) = \\max(0, x),$ [2]\n",
    "\n",
    "A smooth approximation to the rectifier is the *analytic function*: $f(x) = \\ln(1 + e^x)$\n",
    "\n",
    "which is called the **softplus** function.\n",
    "\n",
    "The derivative of softplus is $f'(x) = e^x / (e^x + 1) = 1 / (1 + e^{-x})$, i.e. the **logistic function**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Activation functions\n",
    "\n",
    "<img src=\"../images/activation-functions.png\" width=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Choice of activation functions\n",
    "\n",
    "- ### Driven by induction of non-linearity\n",
    "- ### The gradient of the activation function should be smooth/constant\n",
    "- ### Since DNN learn complex hierarchy of functions, the choice of a particular activation function is almost always driven by experimentation.\n",
    "- ### Yes, the choice of activation function is a hyper-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 51878 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "51878/51878 [==============================] - 3s 56us/step - loss: 1.2040 - acc: 0.6295 - val_loss: 0.8684 - val_acc: 0.7163\n",
      "Epoch 2/30\n",
      "51878/51878 [==============================] - 4s 69us/step - loss: 0.8049 - acc: 0.7219 - val_loss: 0.7453 - val_acc: 0.7373\n",
      "Epoch 3/30\n",
      "51878/51878 [==============================] - 3s 51us/step - loss: 0.7267 - acc: 0.7385 - val_loss: 0.6971 - val_acc: 0.7483\n",
      "Epoch 4/30\n",
      "51878/51878 [==============================] - 2s 43us/step - loss: 0.6897 - acc: 0.7464 - val_loss: 0.6709 - val_acc: 0.7525\n",
      "Epoch 5/30\n",
      "51878/51878 [==============================] - 2s 44us/step - loss: 0.6663 - acc: 0.7500 - val_loss: 0.6516 - val_acc: 0.7573\n",
      "Epoch 6/30\n",
      "51878/51878 [==============================] - 2s 44us/step - loss: 0.6499 - acc: 0.7555 - val_loss: 0.6390 - val_acc: 0.7591\n",
      "Epoch 7/30\n",
      "51878/51878 [==============================] - 2s 46us/step - loss: 0.6375 - acc: 0.7588 - val_loss: 0.6296 - val_acc: 0.7636\n",
      "Epoch 8/30\n",
      "51878/51878 [==============================] - 2s 44us/step - loss: 0.6281 - acc: 0.7619 - val_loss: 0.6190 - val_acc: 0.7655\n",
      "Epoch 9/30\n",
      "51878/51878 [==============================] - 2s 43us/step - loss: 0.6203 - acc: 0.7633 - val_loss: 0.6120 - val_acc: 0.7679\n",
      "Epoch 10/30\n",
      "51878/51878 [==============================] - 2s 43us/step - loss: 0.6142 - acc: 0.7653 - val_loss: 0.6078 - val_acc: 0.7728\n",
      "Epoch 11/30\n",
      "51878/51878 [==============================] - 2s 44us/step - loss: 0.6091 - acc: 0.7668 - val_loss: 0.6034 - val_acc: 0.7698\n",
      "Epoch 12/30\n",
      "51878/51878 [==============================] - 2s 46us/step - loss: 0.6048 - acc: 0.7683 - val_loss: 0.5990 - val_acc: 0.7701\n",
      "Epoch 13/30\n",
      "51878/51878 [==============================] - 2s 43us/step - loss: 0.6009 - acc: 0.7693 - val_loss: 0.5947 - val_acc: 0.7751\n",
      "Epoch 14/30\n",
      "51878/51878 [==============================] - 2s 44us/step - loss: 0.5970 - acc: 0.7703 - val_loss: 0.5932 - val_acc: 0.7737\n",
      "Epoch 15/30\n",
      "51878/51878 [==============================] - 2s 44us/step - loss: 0.5939 - acc: 0.7719 - val_loss: 0.5901 - val_acc: 0.7735\n",
      "Epoch 16/30\n",
      "51878/51878 [==============================] - 2s 45us/step - loss: 0.5911 - acc: 0.7734 - val_loss: 0.5879 - val_acc: 0.7752\n",
      "Epoch 17/30\n",
      "51878/51878 [==============================] - 2s 44us/step - loss: 0.5884 - acc: 0.7733 - val_loss: 0.5827 - val_acc: 0.7776\n",
      "Epoch 18/30\n",
      "51878/51878 [==============================] - 2s 44us/step - loss: 0.5856 - acc: 0.7746 - val_loss: 0.5803 - val_acc: 0.7771\n",
      "Epoch 19/30\n",
      "51878/51878 [==============================] - 2s 44us/step - loss: 0.5833 - acc: 0.7755 - val_loss: 0.5782 - val_acc: 0.7796\n",
      "Epoch 20/30\n",
      "51878/51878 [==============================] - 3s 49us/step - loss: 0.5812 - acc: 0.7771 - val_loss: 0.5767 - val_acc: 0.7798\n",
      "Epoch 21/30\n",
      "51878/51878 [==============================] - 2s 46us/step - loss: 0.5794 - acc: 0.7768 - val_loss: 0.5750 - val_acc: 0.7814\n",
      "Epoch 22/30\n",
      "51878/51878 [==============================] - 2s 45us/step - loss: 0.5775 - acc: 0.7765 - val_loss: 0.5733 - val_acc: 0.7812\n",
      "Epoch 23/30\n",
      "51878/51878 [==============================] - 2s 45us/step - loss: 0.5757 - acc: 0.7794 - val_loss: 0.5722 - val_acc: 0.7822\n",
      "Epoch 24/30\n",
      "51878/51878 [==============================] - 2s 45us/step - loss: 0.5742 - acc: 0.7786 - val_loss: 0.5703 - val_acc: 0.7827\n",
      "Epoch 25/30\n",
      "51878/51878 [==============================] - 2s 42us/step - loss: 0.5725 - acc: 0.7781 - val_loss: 0.5686 - val_acc: 0.7835\n",
      "Epoch 26/30\n",
      "51878/51878 [==============================] - 2s 45us/step - loss: 0.5711 - acc: 0.7782 - val_loss: 0.5676 - val_acc: 0.7827\n",
      "Epoch 27/30\n",
      "51878/51878 [==============================] - 2s 45us/step - loss: 0.5694 - acc: 0.7806 - val_loss: 0.5656 - val_acc: 0.7840\n",
      "Epoch 28/30\n",
      "51878/51878 [==============================] - 2s 44us/step - loss: 0.5684 - acc: 0.7805 - val_loss: 0.5656 - val_acc: 0.7846\n",
      "Epoch 29/30\n",
      "51878/51878 [==============================] - 2s 44us/step - loss: 0.5670 - acc: 0.7810 - val_loss: 0.5646 - val_acc: 0.7867\n",
      "Epoch 30/30\n",
      "51878/51878 [==============================] - 2s 45us/step - loss: 0.5661 - acc: 0.7808 - val_loss: 0.5643 - val_acc: 0.7861\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a200264e0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2 = Sequential()\n",
    "model_2.add(Dense(16, input_shape=(dims,), activation='relu'))\n",
    "model_2.add(Dense(nb_classes, activation='relu'))\n",
    "model_2.add(Activation('softmax'))\n",
    "model_2.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_2.fit(X, Y, epochs=30, batch_size=32, validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 19us/step\n",
      "[0.5642689529418945, 0.7861]\n"
     ]
    }
   ],
   "source": [
    "# Part 3 - Making the predictions and evaluating the model\n",
    "\n",
    "# Predicting the Test set results\n",
    "# Training set, see if the new data probability is right\n",
    "results = model_2.evaluate(X_val, Y_val)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 51878 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "51878/51878 [==============================] - 3s 60us/step - loss: 1.2728 - acc: 0.6050 - val_loss: 0.8674 - val_acc: 0.7264\n",
      "Epoch 2/30\n",
      "51878/51878 [==============================] - 3s 53us/step - loss: 1.0055 - acc: 0.6684 - val_loss: 0.7769 - val_acc: 0.7445\n",
      "Epoch 3/30\n",
      "51878/51878 [==============================] - 3s 54us/step - loss: 0.9577 - acc: 0.6814 - val_loss: 0.7347 - val_acc: 0.7574\n",
      "Epoch 4/30\n",
      "51878/51878 [==============================] - 3s 54us/step - loss: 0.9317 - acc: 0.6868 - val_loss: 0.7150 - val_acc: 0.7603\n",
      "Epoch 5/30\n",
      "51878/51878 [==============================] - 3s 52us/step - loss: 0.9160 - acc: 0.6919 - val_loss: 0.7000 - val_acc: 0.7652\n",
      "Epoch 6/30\n",
      "51878/51878 [==============================] - 3s 53us/step - loss: 0.9008 - acc: 0.6948 - val_loss: 0.6865 - val_acc: 0.7669\n",
      "Epoch 7/30\n",
      "51878/51878 [==============================] - 3s 59us/step - loss: 0.8932 - acc: 0.6986 - val_loss: 0.6777 - val_acc: 0.7695\n",
      "Epoch 8/30\n",
      "51878/51878 [==============================] - 3s 55us/step - loss: 0.8811 - acc: 0.7004 - val_loss: 0.6676 - val_acc: 0.7710\n",
      "Epoch 9/30\n",
      "51878/51878 [==============================] - 3s 52us/step - loss: 0.8712 - acc: 0.7045 - val_loss: 0.6684 - val_acc: 0.7776\n",
      "Epoch 10/30\n",
      "51878/51878 [==============================] - 3s 51us/step - loss: 0.8660 - acc: 0.7061 - val_loss: 0.6549 - val_acc: 0.7769\n",
      "Epoch 11/30\n",
      "51878/51878 [==============================] - 3s 53us/step - loss: 0.8660 - acc: 0.7029 - val_loss: 0.6496 - val_acc: 0.7791\n",
      "Epoch 12/30\n",
      "51878/51878 [==============================] - 3s 55us/step - loss: 0.8502 - acc: 0.7090 - val_loss: 0.6426 - val_acc: 0.7829\n",
      "Epoch 13/30\n",
      "51878/51878 [==============================] - 3s 56us/step - loss: 0.8458 - acc: 0.7117 - val_loss: 0.6394 - val_acc: 0.7822\n",
      "Epoch 14/30\n",
      "51878/51878 [==============================] - 3s 55us/step - loss: 0.8479 - acc: 0.7108 - val_loss: 0.6361 - val_acc: 0.7830\n",
      "Epoch 15/30\n",
      "51878/51878 [==============================] - 3s 54us/step - loss: 0.8445 - acc: 0.7118 - val_loss: 0.6335 - val_acc: 0.7821\n",
      "Epoch 16/30\n",
      "51878/51878 [==============================] - 3s 56us/step - loss: 0.8342 - acc: 0.7139 - val_loss: 0.6287 - val_acc: 0.7864\n",
      "Epoch 17/30\n",
      "51878/51878 [==============================] - 3s 55us/step - loss: 0.8396 - acc: 0.7125 - val_loss: 0.6287 - val_acc: 0.7885\n",
      "Epoch 18/30\n",
      "51878/51878 [==============================] - 3s 55us/step - loss: 0.8357 - acc: 0.7116 - val_loss: 0.6216 - val_acc: 0.7872\n",
      "Epoch 19/30\n",
      "51878/51878 [==============================] - 3s 56us/step - loss: 0.8314 - acc: 0.7156 - val_loss: 0.6205 - val_acc: 0.7911\n",
      "Epoch 20/30\n",
      "51878/51878 [==============================] - 3s 51us/step - loss: 0.8272 - acc: 0.7146 - val_loss: 0.6175 - val_acc: 0.7914\n",
      "Epoch 21/30\n",
      "51878/51878 [==============================] - 3s 52us/step - loss: 0.8246 - acc: 0.7182 - val_loss: 0.6162 - val_acc: 0.7915\n",
      "Epoch 22/30\n",
      "51878/51878 [==============================] - 3s 52us/step - loss: 0.8258 - acc: 0.7170 - val_loss: 0.6103 - val_acc: 0.7907\n",
      "Epoch 23/30\n",
      "51878/51878 [==============================] - 3s 52us/step - loss: 0.8240 - acc: 0.7173 - val_loss: 0.6123 - val_acc: 0.7920\n",
      "Epoch 24/30\n",
      "51878/51878 [==============================] - 3s 52us/step - loss: 0.8194 - acc: 0.7174 - val_loss: 0.6043 - val_acc: 0.7925\n",
      "Epoch 25/30\n",
      "51878/51878 [==============================] - 3s 52us/step - loss: 0.8136 - acc: 0.7219 - val_loss: 0.6036 - val_acc: 0.7914\n",
      "Epoch 26/30\n",
      "51878/51878 [==============================] - 3s 61us/step - loss: 0.8174 - acc: 0.7179 - val_loss: 0.6013 - val_acc: 0.7952\n",
      "Epoch 27/30\n",
      "51878/51878 [==============================] - 3s 61us/step - loss: 0.8160 - acc: 0.7190 - val_loss: 0.6003 - val_acc: 0.7950\n",
      "Epoch 28/30\n",
      "51878/51878 [==============================] - 3s 61us/step - loss: 0.8047 - acc: 0.7231 - val_loss: 0.5994 - val_acc: 0.7964\n",
      "Epoch 29/30\n",
      "51878/51878 [==============================] - 3s 59us/step - loss: 0.8105 - acc: 0.7210 - val_loss: 0.5954 - val_acc: 0.7969\n",
      "Epoch 30/30\n",
      "51878/51878 [==============================] - 3s 56us/step - loss: 0.8087 - acc: 0.7205 - val_loss: 0.5938 - val_acc: 0.7958\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a202e8be0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "model_4 = Sequential()\n",
    "model_4.add(Dense(64, input_shape=(dims,), activation='relu'))\n",
    "model_4.add(Dropout(rate = 0.1))\n",
    "model_4.add(Dense(nb_classes, activation='relu'))\n",
    "model_4.add(Dropout(rate = 0.1))\n",
    "model_4.add(Activation('softmax'))\n",
    "model_4.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy',])\n",
    "model_4.fit(X, Y, epochs=30, batch_size=32, validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 20us/step\n",
      "[0.5938321981430054, 0.7958]\n"
     ]
    }
   ],
   "source": [
    "# Part 3 - Making the predictions and evaluating the model\n",
    "\n",
    "# Predicting the Test set results\n",
    "# Training set, see if the new data probability is right\n",
    "results = model_4.evaluate(X_val, Y_val)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "model_4.save('best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 44096 samples, validate on 7782 samples\n",
      "Epoch 1/20\n",
      "44096/44096 [==============================] - 1s 12us/step - loss: 0.6425 - acc: 0.7617 - val_loss: 0.6457 - val_acc: 0.7584\n",
      "Epoch 2/20\n",
      "44096/44096 [==============================] - 1s 12us/step - loss: 0.6421 - acc: 0.7626 - val_loss: 0.6466 - val_acc: 0.7573\n",
      "Epoch 3/20\n",
      "44096/44096 [==============================] - 1s 12us/step - loss: 0.6418 - acc: 0.7631 - val_loss: 0.6471 - val_acc: 0.7579\n",
      "Epoch 4/20\n",
      "44096/44096 [==============================] - 1s 12us/step - loss: 0.6417 - acc: 0.7628 - val_loss: 0.6475 - val_acc: 0.7579\n",
      "Epoch 5/20\n",
      "44096/44096 [==============================] - 1s 12us/step - loss: 0.6415 - acc: 0.7630 - val_loss: 0.6479 - val_acc: 0.7583\n",
      "Epoch 00005: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x10cade518>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, X_test, Y, Y_test = train_test_split(X, Y, test_size=0.15, random_state=42)\n",
    "\n",
    "fBestModel = 'best_model.h5' \n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=4, verbose=1) \n",
    "best_model = ModelCheckpoint(fBestModel, verbose=0, save_best_only=True)\n",
    "model.fit(X, Y, validation_data = (X_test, Y_test), epochs=20, \n",
    "          batch_size=128, verbose=True, validation_split=0.15, \n",
    "          callbacks=[best_model, early_stop]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The problem of overfitting\n",
    "\n",
    "One of the problems that occur during neural network training is called overfitting. The error on the training set is driven to a very small value, but when new data is presented to the network the error is large. The network has memorized the training examples, but it has not learned to generalize to new situations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Application of dropout to avoid overfitting\n",
    "\n",
    "Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different “thinned” networks.\n",
    "\n",
    "At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Application of dropout to avoid overfitting\n",
    "\n",
    "<img src=\"../images/dropout.png\" width=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Different Neurons are set to zero at different points in time\n",
    "\n",
    "<img src=\"../images/dropout-2.png\" width=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/Recap.png\" alt=\"Recap\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "## Dropout: Summary\n",
    "***\n",
    "***\n",
    "\n",
    "\n",
    "The term \"dropout\" refers to dropping out units (both hidden and visible) in a neural network.\n",
    "It is a very efficient way of performing model averaging with neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Comparison of different networks\n",
    "\n",
    "<img src=\"../images/comparision.png\" width=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Observations\n",
    "\n",
    "- ## Deeper is not always better\n",
    "- ## Set architecture based on complexity of the problem\n",
    "- ## Use techniques such as 'dropout' to avoid overfitting on training set\n",
    "- ## Always evaluate on test set before putting models into production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Take aways\n",
    "\n",
    "- ## Multi-class classification using soft-max and cross-entropy\n",
    "- ## Backpropagation algorithm\n",
    "- ## Overfitting and dropout\n",
    "- ## Experimentation over different depth DNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
